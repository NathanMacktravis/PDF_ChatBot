## Importation des biblioth√®ques n√©cessaires
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  # Pour le chargement du tokenizer et du mod√®le de transformation
from transformers import pipeline  # Pour cr√©er un pipeline de g√©n√©ration de texte
import torch  # Pour g√©rer les tensors et les mod√®les sur CPU ou GPU
from langchain.embeddings import SentenceTransformerEmbeddings  # Pour g√©n√©rer des embeddings de texte
from langchain.vectorstores import Chroma  # Pour cr√©er une base de donn√©es vectorielle
from langchain.chains import RetrievalQA  # Pour cr√©er une cha√Æne de question-r√©ponse bas√©e sur la r√©cup√©ration
from langchain.llms import HuggingFacePipeline  # Pour cr√©er un mod√®le de LLM personnalis√© √† partir de pipelines HuggingFace
from constants import CHROMA_SETTINGS  # Importation des param√®tres sp√©cifiques √† Chroma (fichier constants.py)
import os 
import shutil  # Pour manipuler les fichiers et les dossiers

## Configuration du p√©riph√©rique (GPU si disponible, sinon CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
checkpoint = "LaMini-T5-738M"  # Mod√®le pr√©-entra√Æn√© choisi pour la t√¢che

## Chargement du tokenizer et du mod√®le depuis les fichiers pr√©-entra√Æn√©s
tokenizer = AutoTokenizer.from_pretrained(checkpoint)  
base_model = AutoModelForSeq2SeqLM.from_pretrained(
    checkpoint, 
    torch_dtype = torch.float32  # Sp√©cification du type de donn√©es utilis√© par le mod√®le
)

## Fonction pour cr√©er le pipeline de g√©n√©ration de texte avec le mod√®le HuggingFace
def llm_pipeline():
    pipe = pipeline(
        'text2text-generation',  # Type de t√¢che : g√©n√©ration de texte √† partir de texte
        model = base_model,  # Mod√®le √† utiliser pour la g√©n√©ration
        tokenizer = tokenizer,  # Tokenizer associ√© au mod√®le
        max_length = 512,  # Longueur maximale des s√©quences g√©n√©r√©es
        do_sample = True,  # Autorisation de l'√©chantillonnage lors de la g√©n√©ration (g√©n√©ration plus vari√©e)
        temperature = 0.3,  # Contr√¥le de la cr√©ativit√© (valeur plus basse = plus conservateur)
        top_p= 0.95,  # Seuil pour la m√©thode de d√©codage top-p (ou nucleus sampling)
    )
    local_llm = HuggingFacePipeline(pipeline=pipe)  # Enveloppement du pipeline dans un objet LangChain
    return local_llm

## Fonction pour cr√©er un objet de question-r√©ponse bas√© sur la r√©cup√©ration (RetrievalQA)
def qa_llm():
    # Supprimer l'index existant pour √©viter les probl√®mes de corruption
    if os.path.exists("db"):
        shutil.rmtree("db")  # Supprime le dossier "db" s'il existe

    llm = llm_pipeline()  # Cr√©ation du pipeline de g√©n√©ration de texte
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")  # G√©n√©ration d'embeddings √† partir du mod√®le SentenceTransformer
    db = Chroma(persist_directory="db", embedding_function = embeddings, client_settings=CHROMA_SETTINGS)  # Cr√©ation de la base de donn√©es vectorielle avec persistance des donn√©es
    retriever = db.as_retriever()  # Conversion de la base de donn√©es en un objet de r√©cup√©ration de documents
    qa = RetrievalQA.from_chain_type(
        llm = llm,  # Utilisation du LLM pour g√©n√©rer des r√©ponses
        chain_type = "stuff",  # Type de cha√Æne utilis√© (ici, une cha√Æne simple)
        retriever = retriever,  # Syst√®me de r√©cup√©ration pour trouver les documents pertinents
        return_source_documents=True  # Retourne les documents sources avec les r√©ponses
    )
    return qa

## Fonction pour traiter la r√©ponse √† une question donn√©e
def process_answer(instruction):
    qa = qa_llm()  # Initialisation de l'objet de question-r√©ponse
    generated_text = qa(instruction)  # G√©n√©ration de la r√©ponse √† partir de l'instruction (question)
    answer = generated_text['result']  # Extraction de la r√©ponse g√©n√©r√©e
    return answer

# Fonction principale pour interagir avec l'utilisateur via la console
def main():
    print("Question & Answer app PDF ü¶úüìÑ")  # Titre de l'application
    print("This is a power Generative Question & Answer App that responds to questions about your PDF file.")  # Description de l'application
    
    while True:
        question = input("Enter your question (or type 'exit' to quit): ")  # Entr√©e de la question par l'utilisateur
        if question.lower() == 'exit':  # Condition pour quitter la boucle
            break
        answer = process_answer(question)  # Traitement de la question et obtention de la r√©ponse
        print(f"Your question: {question}")  # Affichage de la question pos√©e
        print(f"Answer: {answer}")  # Affichage de la r√©ponse obtenue


if __name__ == "__main__":
    main()  